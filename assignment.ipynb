{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7SXpaKwwGe5x"
   },
   "source": [
    "# TM10007 Assignment template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "CiDn2Sk-VWqE",
    "outputId": "64224cd2-6054-4b04-a3f6-af8290400dfc"
   },
   "outputs": [],
   "source": [
    "# Run this to use from colab environment\n",
    "#!pip install -q --upgrade git+https://github.com/jveenland/tm10007_ml.git\n",
    "\n",
    "# General packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets as ds\n",
    "import seaborn\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn import model_selection\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.metrics.pairwise import rbf_kernel, sigmoid_kernel\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold \n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_validate\n",
    "from scipy.stats import randint\n",
    "\n",
    "#moeten deze nog even opschonen als alle code erin staat "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and cleaning\n",
    "\n",
    "Below are functions to load the dataset of your choice. After that, it is all up to you to create and evaluate a classification method. Beware, there may be missing values in these datasets. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-NE_fTbKGe5z"
   },
   "outputs": [],
   "source": [
    "from worclipo.load_data import load_data\n",
    "\n",
    "data = load_data()\n",
    "print(f'The number of samples: {len(data.index)}')\n",
    "print(f'The number of columns: {len(data.columns)}')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after checking for duplicates: 115\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Check for duplicate patients\"\"\" \n",
    "duplicates = data[data.duplicated(keep=False)]  \n",
    "data = data.drop_duplicates(keep='first')\n",
    "print(f\"Number of rows after checking for duplicates: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing values\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Check for missing values\"\"\"\n",
    "data_missing_values = data.replace(' ', np.nan)                   \n",
    "missing_values = data[data.isnull().any(axis=1)]         \n",
    "\n",
    "if not missing_values.empty:\n",
    "    print(f\"Missing values; {missing_values}\")\n",
    "else:\n",
    "    print(\"No missing values\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of samples train: 92\n",
      "The number of columns train: 494\n",
      "The number of samples test: 23\n",
      "The number of columns test: 494\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Splitting the data into a test and training dataset: outer cross-validation\"\"\"\n",
    "data_train, data_test = model_selection.train_test_split(data, test_size=0.20, random_state=42)\n",
    "print(f'The number of samples train: {len(data_train.index)}')\n",
    "print(f'The number of columns train: {len(data_train.columns)}')\n",
    "print(f'The number of samples test: {len(data_test.index)}')\n",
    "print(f'The number of columns test: {len(data_test.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of captured outliers in the train data: 676\n",
      "\n",
      "Total number of captured outliers in the test data: 130\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Detect and replace outliers\"\"\"\n",
    "from outliers import outlier_detection\n",
    "data_train, total_outliers = outlier_detection(data_train)\n",
    "print(\"\\nTotal number of captured outliers in the train data:\", total_outliers)\n",
    "\n",
    "# dit moet dus wellicht nog op basis van de outliers van de train data \n",
    "data_test, total_outliers = outlier_detection(data_test)\n",
    "print(\"\\nTotal number of captured outliers in the test data:\", total_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prepro_data import processing_data_scaling\n",
    "from prepro_data import processing_data_scaling_test\n",
    "#This data is scaled and a variance and correlation threshold are applied\n",
    "\n",
    "#Train data\n",
    "data_merged_scaling, df_label_scaling, df_processed_scaling, variance_data, selected_columns, drop_correlation, scaler = processing_data_scaling(data_train)\n",
    "print(data_merged_scaling)\n",
    "\n",
    "#Test data\n",
    "data_merged_scaling_test, df_label_scaling_test, df_processed_scaling_test = processing_data_scaling_test(data_test, variance_data, selected_columns, drop_correlation, scaler)\n",
    "print(data_merged_scaling_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prepro_data import processing_data_rfecv\n",
    "from prepro_data import processing_data_rfecv_test\n",
    "#This data is scaled, a variance and correlation threshold are applied,\n",
    "# and RFECV brought back the number of features to 18 (takes a bit longer to run)\n",
    "\n",
    "#Train data\n",
    "data_rfecv, df_label, df_processed, variance_data, selected_columns, drop_correlation, scaler, rfecv, selected_features = processing_data_rfecv(data_train)\n",
    "print(data_rfecv)\n",
    "\n",
    "#Test data\n",
    "data_rfecv_test, df_label_rfecv_test, df_processed_rfecv_test = processing_data_rfecv_test(data_test, variance_data, selected_columns, drop_correlation, scaler, rfecv, selected_features)\n",
    "print(data_rfecv_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 123 candidates, totalling 615 fits\n",
      "Best hyperparameters: {'C': 0.1, 'kernel': 'linear'}\n",
      "Training AUC: 0.9381\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Support Vector Machine\"\"\"\n",
    "# Split data into features and labels\n",
    "X = data_rfecv.drop(data_rfecv.columns[[0, 1]], axis=1)  # Features\n",
    "Y = df_label['label']  # Labels\n",
    "\n",
    "# Set up the parameter grid for GridSearchCV\n",
    "param_grid = [\n",
    "    {'kernel': ['linear'], 'C': [0.1, 1, 10]},\n",
    "    {'kernel': ['rbf'], 'C': [0.1, 1, 10], 'gamma': ['scale', 'auto', 0.1, 1]},\n",
    "    {'kernel': ['poly'], 'C': [0.1, 1, 10], 'degree': [3, 4, 5], 'gamma': ['scale', 'auto', 0.1, 1], 'coef0': [0.0, 0.1, 0.5]},\n",
    "]\n",
    "\n",
    "# Set up the SVM classifier\n",
    "svm_classifier = SVC(random_state=42, probability=True)  # Enable probability for AUC calculation\n",
    "\n",
    "# Set up inner cross-validation for hyperparameter tuning\n",
    "inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Set up GridSearchCV to search for the best hyperparameters\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=svm_classifier,\n",
    "    param_grid=param_grid,\n",
    "    scoring='roc_auc',         # Optimize AUC score\n",
    "    refit=True,                # Refit best model on full training data\n",
    "    cv=inner_cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Perform the grid search on the training data to find the best hyperparameters\n",
    "grid_search.fit(X, Y)\n",
    "\n",
    "# Get the best model (with the best hyperparameters)\n",
    "best_model_svm = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions to evaluate performance\n",
    "y_pred = best_model_svm.predict(X)\n",
    "y_pred_proba = best_model_svm.predict_proba(X)[:, 1]  # Probability estimates for AUC\n",
    "\n",
    "# Convert categorical labels to numerical labels (0 or 1)\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)  # 'lipoma' -> 0, 'liposarcoma' -> 1\n",
    "\n",
    "# Ensure predictions are also numerical\n",
    "y_pred = best_model_svm.predict(X)\n",
    "y_pred = le.transform(y_pred)  # Convert categorical predictions ('lipoma', 'liposarcoma') to 0 and 1\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "auc_score = roc_auc_score(Y, y_pred_proba)\n",
    "\n",
    "# Print results\n",
    "print(f\"Best hyperparameters: {grid_search.best_params_}\")\n",
    "print(f\"Training AUC: {auc_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Best hyperparameters: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 200}\n",
      "Training AUC: 0.9783\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Random Forest Classifier\"\"\"\n",
    "# Split data into features and labels\n",
    "X = data_rfecv.drop(data_rfecv.columns[[0, 1]], axis=1)  # Features\n",
    "Y = df_label['label']  # Labels\n",
    "\n",
    "# Convert categorical labels to numerical labels (0 or 1)\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)  # e.g., 'lipoma' -> 0, 'liposarcoma' -> 1\n",
    "\n",
    "# Set up parameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_leaf': [4, 5, 10],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Inner CV for hyperparameter tuning\n",
    "inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Set up GridSearchCV to search for best hyperparameters\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf_classifier,\n",
    "    param_grid=param_grid,\n",
    "    scoring='roc_auc',\n",
    "    refit=True,\n",
    "    cv=inner_cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X, Y)\n",
    "\n",
    "# Best model after Grid Search\n",
    "best_model_rf = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions\n",
    "y_pred = best_model_rf.predict(X)\n",
    "y_pred_proba = best_model_rf.predict_proba(X)[:, 1]  # Probability for positive class\n",
    "\n",
    "# Evaluate model performance\n",
    "auc_score = roc_auc_score(Y, y_pred_proba)\n",
    "\n",
    "# Print results\n",
    "print(f\"Best hyperparameters: {grid_search.best_params_}\")\n",
    "print(f\"Training AUC: {auc_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best hyperparameters: {'metric': 'manhattan', 'n_neighbors': 15, 'weights': 'distance'}\n",
      "Training AUC: 1.0000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"KNNeighbour classifier\"\"\"\n",
    "# Split data into features and labels\n",
    "X = data_rfecv.drop(data_rfecv.columns[[0, 1]], axis=1)  # Features (excluding first two columns)\n",
    "Y = df_label['label']  # Labels\n",
    "\n",
    "# Set up the parameter grid for GridSearchCV (for KNN, we mainly tune 'n_neighbors', 'weights', and 'metric')\n",
    "param_grid = {\n",
    "    'n_neighbors': [5, 7, 10, 15, 20],  # Trying different values for k (number of neighbors)\n",
    "    'weights': ['uniform', 'distance'],  # Weighting of neighbors\n",
    "    'metric': ['euclidean', 'manhattan']  # Distance metrics to use\n",
    "}\n",
    "\n",
    "# Set up the KNN classifier\n",
    "knn_classifier = KNeighborsClassifier()  # KNN classifier\n",
    "\n",
    "# Set up inner cross-validation for hyperparameter tuning\n",
    "inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Set up GridSearchCV to search for the best hyperparameters\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=knn_classifier, \n",
    "    param_grid=param_grid, \n",
    "    cv=inner_cv, \n",
    "    n_jobs=-1, \n",
    "    verbose=1, \n",
    "    scoring='roc_auc',         # Optimize AUC score\n",
    "    refit=True,                # Refit best model on full training data\n",
    ")\n",
    "\n",
    "# Perform the grid search on the training data to find the best hyperparameters\n",
    "grid_search.fit(X, Y)\n",
    "\n",
    "# Get the best model (with the best hyperparameters)\n",
    "best_model_knn = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions to evaluate performance\n",
    "y_pred = best_model_knn.predict(X)\n",
    "y_pred_proba = best_model_knn.predict_proba(X)[:, 1]  # Probability estimates for AUC\n",
    "\n",
    "# Convert categorical labels to numerical labels (0 or 1)\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)  # 'lipoma' -> 0, 'liposarcoma' -> 1\n",
    "\n",
    "# Ensure predictions are also numerical\n",
    "y_pred = best_model_knn.predict(X)\n",
    "y_pred = le.transform(y_pred)  # Convert categorical predictions ('lipoma', 'liposarcoma') to 0 and 1\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "auc_score = roc_auc_score(Y, y_pred_proba)\n",
    "\n",
    "# Print results\n",
    "print(f\"Best hyperparameters: {grid_search.best_params_}\")\n",
    "print(f\"Training AUC: {auc_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Testing the classifiers on the test data\"\"\"\n",
    "#use best_model_svm + best_model_rf + best_model_knn for this\n",
    "#use this scores: F1 score, AUC, accuracy, precision, recall "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "KT-2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
